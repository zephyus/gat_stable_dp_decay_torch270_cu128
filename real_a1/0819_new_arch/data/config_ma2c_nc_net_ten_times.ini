[MODEL_CONFIG]
rmsp_alpha = 0.99
rmsp_epsilon = 1e-5
max_grad_norm = 40  
;40 -> 10 ->1.0 or 5.0
gamma = 0.99
lr_init = 5e-4
 ; 0416 5e-4  1e-4
lr_decay = constant
entropy_coef = 0.01
value_coef = 0.5
num_lstm = 64
num_fc = 64
batch_size = 120
; same nodes may be updated by ~28 propagations,
; adjust norm accordingly
reward_norm = 2000.0
reward_clip = -1

; 0502_first_50000k,linear decay
gat_dropout_init = 0.2
gat_dropout_final = 0.1
gat_dropout_decay_steps = 500000

[TRAIN_CONFIG]
total_step = 1e6
test_interval = 1e5
log_interval = 1e3

[ENV_CONFIG]
clip_wave = -1
clip_wait = -1
control_interval_sec = 5
; agent is greedy, ia2c, ia2c_fp, ma2c_som, ma2c_ic3, ma2c_nc.
agent = ma2c_nc
; coop discount is used to discount the neighbors' impact
coop_gamma = 1.0
data_path = ./envs/real_net_data/
episode_length_sec = 3600
; for realnet, the normailization is done per agent
norm_wave = 1.0
norm_wait = -1
coef_wait = 0
flow_rate = 325
; objective is chosen from queue, wait, hybrid
objective = queue
bonus_factor = 0.0
scenario = atsc_real_net
seed = 12
test_seeds = 10000
yellow_interval_sec = 2